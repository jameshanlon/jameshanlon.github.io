<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="author" content="James W. Hanlon">
  <title>New chips for machine intelligence</title>
  <link rel="icon" href="./favicon.png" sizes="16x16" type="image/png">
  <link rel="stylesheet" type="text/css" href="./theme/css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="./theme/css/lightbox.min.css">
  <link rel="stylesheet" type="text/css" href="./theme/css/main.css"/>
  <link href="http://jameswhanlon.com/reeds/atom.xml"
      type="application/atom+xml" rel="alternate"
      title="James W. Hanlon Atom Feed" />
  <link href="http://jameswhanlon.com/reeds/rss.xml"
      type="application/rss+xml" rel="alternate"
      title="James W. Hanlon RSS Feed" />
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>
</head>
<body>
  <header>
  <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
    <div class="container-fluid">
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarCollapse">
        <ul class="navbar-nav me-auto mb-2 mb-md-0 text-uppercase">
          <li class="nav-item">
              <a class="nav-link" href="/index.html">notes</a>
          </li>
          <li class="nav-item">
              <a class="nav-link" href="/projects.html">projects</a>
          </li>
          <li class="nav-item">
              <a class="nav-link" href="/archive.html">archive</a>
          </li>
          <li class="nav-item">
              <a class="nav-link" href="/about.html">about</a>
          </li>
        </ul>
        <a class="navbar-brand" href="#">James W. Hanlon</a>
      </div>
    </div>
  </nav>
  </header>

  <main class="flex-shrink-0">
  <div class="container">
  <h1>
    New chips for machine&nbsp;intelligence
  </h1>
  <div class="lead">
    <time class="published" datetime="2019-10-04T00:00:00+02:00">
      04 Oct 2019
    </time><br>
      <span class="article-tag small"><a href="/tag/computing.html">computing</a></span>
      <span class="article-tag small"><a href="/tag/machine-intelligence.html">machine-intelligence</a></span>
      <span class="article-tag small"><a href="/tag/computer-architecture.html">computer-architecture</a></span>
  </div>
  <div class="article-body">
    <p>This note summarises details of some of the new silicon chips for machine
intelligence. Its aim is to distil the most important implementation and
architectural details (at least that are currently available), to highlight the
main differences between them. I&#8217;m focusing on chips designed for training
since they represent the frontier in performance and capability. There are many
chips designed for inference, but these are typically intended for use in
embedded or edge&nbsp;deployments.</p>
<p>In&nbsp;summary:</p>
<table class="table table-sm">
<caption>
*Speculated<br>
&dagger; Figures given for a single chip
</caption>
<thead>
  <tr>
    <th scope="col">Chip</th>
    <th scope="col">Process</th>
    <th scope="col">Die size<br>mm<sup>2</sup></th>
    <th scope="col"><span class="caps">TDP</span><br>(W)</th>
    <th scope="col">On-chip <span class="caps">RAM</span><br>(<span class="caps">MB</span>)</th>
    <th scope="col">Peak <span class="caps">FP32</span><br>(TFLOPs)</th>
    <th scope="col">Peak <span class="caps">FP16</span><br>(TFLOPs)</th>
    <th scope="col">Mem b/w<br>(<span class="caps">GB</span>/s)</th>
    <th scope="col"><span class="caps">IO</span> b/w<br>(<span class="caps">GB</span>/s)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td><a href="#cerebras">Cerebras <span class="caps">WSE</span><sup>&dagger;</sup></a></td>
    <td><span class="caps">TSMC</span> 16nm</td>
    <td>510</td>
    <td>180</td>
    <td>225</td>
    <td>40.6</td>
    <td>n/a</td>
    <td>0</td>
    <td>Unknown</td>
  </tr>
  <tr>
    <td><a href="#google-tpu-1">Google <span class="caps">TPU</span> v1</a></td>
    <td>28nm</td>
    <td>Unknown</td>
    <td>75</td>
    <td>28</td>
    <td>n/a</td>
    <td>23 (<span class="caps">INT16</span>)</td>
    <td>30 (<span class="caps">DDR3</span>)</td>
    <td>14</td>
  </tr>
  <tr>
    <td><a href="#google-tpu-2">Google <span class="caps">TPU</span> v2</a></td>
    <td>20nm*</td>
    <td>Unknown</td>
    <td>200*</td>
    <td>Unknown</td>
    <td>Unknown</td>
    <td>45</td>
    <td>600 (<span class="caps">HBM</span>)</td>
    <td>8*</td>
  </tr>
  <tr>
    <td><a href="#google-tpu-3">Google <span class="caps">TPU</span> v3</a></td>
    <td>16/12nm*</td>
    <td>Unknown</td>
    <td>200*</td>
    <td>Unknown</td>
    <td>Unknown</td>
    <td>90</td>
    <td>1200 (<span class="caps">HBM2</span>)*</td>
    <td>8*</td>
  </tr>
  <tr>
    <td><a href="#graphcore-c2-ipu">Graphcore <span class="caps">IPU</span></a></td>
    <td>16nm</td>
    <td>800*</td>
    <td>150</td>
    <td>300</td>
    <td>Unknown</td>
    <td>125</td>
    <td>0</td>
    <td>384</td>
  </tr>
  <tr>
    <td><a href="#habana-gaudi">Habana Gaudi</a></td>
    <td><span class="caps">TSMC</span> 16nm</td>
    <td>500*</td>
    <td>300</td>
    <td>Unknown</td>
    <td>Unknown</td>
    <td>Unknown</td>
    <td>1000 (<span class="caps">HBM2</span>)</td>
    <td>250</td>
  </tr>
  <tr>
    <td><a href="#huawei-ascend">Huawei Ascend 910</a></td>
    <td>7nm+ <span class="caps">EUV</span></td>
    <td>456</td>
    <td>350</td>
    <td>64</td>
    <td>Unknown</td>
    <td>256</td>
    <td>1200 (<span class="caps">HBM2</span>)</td>
    <td>115</td>
  </tr>
  <tr>
    <td><a href="#intel-nnp-t">Intel <span class="caps">NNP</span>-T</a></td>
    <td><span class="caps">TSMC</span> <span class="caps">16FF</span>+</td>
    <td>688</td>
    <td>250</td>
    <td>60</td>
    <td>Unknown</td>
    <td>110</td>
    <td>1220 (<span class="caps">HBM2</span>)</td>
    <td>447</td>
  </tr>
  <tr>
    <td><a href="#nvidia-volta">Nvidia Volta</a></td>
    <td><span class="caps">TSMC</span> 12nm <span class="caps">FFN</span></td>
    <td>815</td>
    <td>300</td>
    <td>21.1</td>
    <td>15.7</td>
    <td>125</td>
    <td>900 (<span class="caps">HBM2</span>)</td>
    <td>300</td>
  </tr>
  <tr>
    <td><a href="#nvidia-turing">Nvidia Turing</a></td>
    <td><span class="caps">TSMC</span> 12nm <span class="caps">FFN</span></td>
    <td>754</td>
    <td>250</td>
    <td>24.6</td>
    <td>16.3</td>
    <td>130.5</td>
    <td>672 (<span class="caps">HBM2</span>)</td>
    <td>100</td>
  </tr>
</tbody>
</table>

<p><a name="cerebras" class="anchor"></a></p>
<h2>Cerebras Wafer-Scale&nbsp;Engine</h2>
<p><img class="float-right img-fluid" src="thumbs/cerebras-wse_220x220.png" alt="Cerebras chip"></p>
<p>The Cerebras Wafer-Scale Engine (<span class="caps">WSE</span>) is undoubtedly the most bold and
innovative design to appear recently. Wafer-scale integration is not a new
idea, but integration issues to do with yield, power delivery and thermal
expansion have made it difficult to commercialise (see the 1989 <a href="http://www.computinghistory.org.uk/det/3043/Anamartic-Wafer-Scale-160MB-Solid-State-Disk/">Anamartic 160
<span class="caps">MB</span> solid state disk</a>). Cerebras use this approach to integrate 84
chips with high-speed interconnect, uniformly scaling the 2D-mesh based
interconnect to huge proportions. This provides a machine with a large amount
of memory (18 <span class="caps">GB</span>) distributed among a large amount of compute (3.3 Peta FLOPs
peak). It is unclear how this architecture scales beyond single WSEs; the
current trend in neural nets is to larger networks with billions of weights,
which will necessitate such&nbsp;scaling.</p>
<p>General&nbsp;details:</p>
<ul>
<li>Announced August&nbsp;2019.</li>
<li>46,225 mm<sup>2</sup> wafer-scale integrated system (215 mm x 215 mm) om <span class="caps">TSMC</span> 16&nbsp;nm.</li>
<li>1.2T&nbsp;transistors.</li>
<li>Many individual chips: a total of 84 (12 wide by 7&nbsp;tall).</li>
<li>18 <span class="caps">GB</span> total of <span class="caps">SRAM</span> memory, distributed among&nbsp;cores.</li>
<li>426,384 simple compute&nbsp;cores.</li>
<li>Silicon defects can be repaired by using redundant cores and links to bypass a faulty area. It appears that each column includes one redundant core, leaving 410,592 functional&nbsp;cores.</li>
<li>Speculated clock speed of ~1 GHz and 15 kW power&nbsp;consumption.</li>
</ul>
<p>Interconnect and <span class="caps">IO</span>:</p>
<ul>
<li>Interconnections between chips, across scribe lines, with wiring added in post-processing steps after conventional wafer&nbsp;manufacturing.</li>
<li>IOs brought out on east and west edges of wafer, which is limited by the pad density on each edge. It is unlikely there are any high-speed SerDes since these would need to be integrated in every chip, making a sizeable part of the wafer area redundant apart from chips with edges on the&nbsp;periphery.</li>
<li>2D mesh-based interconnect, supports single-word messages. According to their whitepaper: &#8220;The Cerebras software configures all the cores on the <span class="caps">WSE</span> to support the precise communication required&#8221; indicating that the interconnect is statically configured to support a fixed communication&nbsp;pattern.</li>
<li>Interconnect requires static configuration to support specific patterns of&nbsp;communication.</li>
<li>Zeros not transmitted on the interconnect to optimise for&nbsp;sparsity.</li>
</ul>
<p>Each&nbsp;core:</p>
<ul>
<li>Is ~0.1 mm<sup>2</sup> of&nbsp;silicon.</li>
<li>Has 47 kB <span class="caps">SRAM</span>&nbsp;memory.</li>
<li>Zeros not loaded from memory and zeros not&nbsp;multiplied.</li>
<li>Assumed <span class="caps">FP32</span> precision and scalar execution (can&#8217;t filter zeros from memory with <span class="caps">SIMD</span>).</li>
<li><span class="caps">FMAC</span> datapath (peak 8 operations per&nbsp;cycle).</li>
<li>Tensor control unit to feed the <span class="caps">FMAC</span> datapath with strided accesses from memory or inbound data from&nbsp;links.</li>
<li>Has four 8 <span class="caps">GB</span>/s bidirectional links to its&nbsp;neighbours.</li>
</ul>
<p>Each&nbsp;die:</p>
<ul>
<li>Is 17 mm x 30 mm = 510 mm<sup>2</sup> of&nbsp;silicon.</li>
<li>Has 225 <span class="caps">MB</span> <span class="caps">SRAM</span>&nbsp;memory.</li>
<li>Has 54 x 94 = 5,076 cores (two cores per row/column possibly unused due to repair scheme leaving 4,888 usable&nbsp;cores).</li>
<li>Peak <span class="caps">FP32</span> performance of 40 Tera&nbsp;FLOPs.</li>
</ul>
<p>References:</p>
<ul>
<li><a href="https://www.anandtech.com/show/14758/hot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning">AnandTech report from <span class="caps">HC31</span>, August&nbsp;2019</a></li>
<li><a href="https://www.cerebras.net/wp-content/uploads/2019/08/Cerebras-Wafer-Scale-Engine-Whitepaper.pdf">Cerebras white paper, August 2019 (no longer&nbsp;available)</a></li>
<li><a href="https://www.cerebras.net/cerebras-wafer-scale-engine-why-we-need-big-chips-for-deep-learning/">Cerebras Wafer Scale Engine: Why we need big chips for Deep&nbsp;Learning</a></li>
</ul>
<p><a name="google-tpu-3" class="anchor"></a></p>
<h2>Google <span class="caps">TPU</span>&nbsp;v3</h2>
<p><img class="float-right img-fluid" src="thumbs/google-tpu3_220x220.png" alt="TPU-3 board"></p>
<p>With few details available on the specifications of the <span class="caps">TPU</span> v3, it is likely an
incremental improvement to the <span class="caps">TPU</span> v2: doubling the performance, adding <span class="caps">HBM2</span>
memory to double the capacity and&nbsp;bandwidth.</p>
<p>General details (per&nbsp;chip):</p>
<ul>
<li>Announced May&nbsp;2018.</li>
<li>Likely to be 16nm or&nbsp;12nm.</li>
<li>200W estimated <span class="caps">TDP</span>.</li>
<li>105 TFLOPs of BFloat16, likely from doubling the MXUs to&nbsp;four.</li>
<li>Each <span class="caps">MXU</span> has dedicated access to 8 <span class="caps">GB</span> of&nbsp;memory.</li>
<li>Integrated in four-chip modules (pictured), 420 TFLOPs peak&nbsp;performance.</li>
</ul>
<p><span class="caps">IO</span>:</p>
<ul>
<li>32 <span class="caps">GB</span> <span class="caps">HBM2</span> integrated memory with access bandwidth of 1200 GBps&nbsp;(assumed).</li>
<li>PCIe-3 x8 assumed at 8&nbsp;GBps.</li>
</ul>
<p>References:</p>
<ul>
<li><a href="https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/">NextPlatform Tearing apart google’s <span class="caps">TPU</span> 3.0 <span class="caps">AI</span>&nbsp;coprocessor</a></li>
</ul>
<p><a name="google-tpu-2" class="anchor"></a></p>
<h2>Google <span class="caps">TPU</span>&nbsp;v2</h2>
<p><img class="float-right img-fluid" src="thumbs/google-tpu2_220x220.png" alt="TPU-2 board"></p>
<p>The <span class="caps">TPU</span> v2 is designed for training and inference. It improves over the <span class="caps">TPU</span> v1
with floating point arithmetic and enhanced memory capacity and bandwidth with
<span class="caps">HBM</span> integrated&nbsp;memory.</p>
<p>General details (per&nbsp;chip):</p>
<ul>
<li>Announced May&nbsp;2017.</li>
<li>Likely to be&nbsp;20nm.</li>
<li>200-250W estimated <span class="caps">TDP</span>.</li>
<li>45 TFLOPs of&nbsp;BFloat16. </li>
<li>Two cores with scalar and matrix&nbsp;units.</li>
<li>Also supports <span class="caps">FP32</span>.</li>
<li>Integrated in four-chip modules (pictured), 180 TFLOPs peak&nbsp;performance.</li>
</ul>
<p>Each&nbsp;core:</p>
<ul>
<li>128x128x32b systolic matrix unit (<span class="caps">MXU</span>) with BFloat16 multiplication and <span class="caps">FP32</span>&nbsp;accumulation.</li>
<li><span class="caps">8GB</span> of dedicated <span class="caps">HBM</span> with access bandwidth of 300&nbsp;GBps.</li>
<li>Peak throughput of 22.5 TFLOPs of&nbsp;BFloat16.</li>
</ul>
<p><span class="caps">IO</span>:</p>
<ul>
<li>16 <span class="caps">GB</span> <span class="caps">HBM</span> integrated memory at 600 GBps bandwidth&nbsp;(assumed).</li>
<li>PCIe-3 x8 (8&nbsp;GBps).</li>
</ul>
<p>References:</p>
<ul>
<li><a href="https://www.nextplatform.com/2017/05/22/hood-googles-tpu2-machine-learning-clusters/">NextPlatform Under The Hood Of Google’s <span class="caps">TPU2</span> Machine Learning Clusters, May&nbsp;2017</a></li>
<li><a href="https://www.tomshardware.com/news/tpu-v2-google-machine-learning,35370.html">Tom&#8217;s Hardware: Hot Chips 2017: A Closer Look At Google&#8217;s <span class="caps">TPU</span> v2, September&nbsp;2017</a></li>
</ul>
<p><a name="google-tpu-1" class="anchor"></a></p>
<h2>Google <span class="caps">TPU</span>&nbsp;v1</h2>
<p><img class="float-right img-fluid" src="thumbs/google-tpu1_220x220.png" alt="TPU-1 board"></p>
<p>Google&#8217;s first generation <span class="caps">TPU</span> was designed for inference only and supports only
integer arithmetic. It provides acceleration to a host <span class="caps">CPU</span> by being sent
instructions across PCIe-3, to perform matrix multiplications and apply
activation functions. This is a significant simplification which would have
saved much time in design and&nbsp;verification.</p>
<p>General&nbsp;details:</p>
<ul>
<li>Announced in&nbsp;2016.</li>
<li>331 mm<sup>2</sup> die on 28nm&nbsp;process.</li>
<li>Clocked at 700 MHz and 28-40W <span class="caps">TDP</span>.</li>
<li>28 <span class="caps">MB</span> on-chip <span class="caps">SRAM</span> memory: 24 <span class="caps">MB</span> for activations and 4 <span class="caps">MB</span> for&nbsp;accumulators.</li>
<li>Proportions of the die area: 35% memory, 24% matrix multiply unit, 41%
  remaining area for&nbsp;logic.</li>
<li>256x256x8b systolic matrix multiply unit (64K&nbsp;MACs/cycle).</li>
<li><span class="caps">INT8</span> and <span class="caps">INT16</span> arithmetic (peak 92 and 23 TOPs/s&nbsp;respectively).</li>
</ul>
<p><span class="caps">IO</span>:</p>
<ul>
<li>8 <span class="caps">GB</span> <span class="caps">DDR3</span>-2133 <span class="caps">DRAM</span> accessible via two ports at 34 <span class="caps">GB</span>/s.</li>
<li>PCIe-3 x 16 (14&nbsp;GBps).</li>
</ul>
<p>References:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Tensor_processing_unit">Wikipedia: Tensor Processing&nbsp;Unit</a></li>
<li><a href="https://arxiv.org/abs/1704.04760"><span class="caps">TPU</span> v1 paper (In-Datacenter Performance Analysis of a Tensor Processing Unit), April&nbsp;2017</a></li>
</ul>
<p><a name="graphcore-c2-ipu" class="anchor"></a></p>
<h2>Graphcore <span class="caps">IPU</span></h2>
<p><img class="float-right img-fluid" src="thumbs/graphcore-ipu_220x220.png" alt="Graphcore IPU floorplan"></p>
<p><strong><span class="caps">DISCLAIMER</span></strong>: I work at Graphcore, and all of the information given here is
lifted directly from the linked references&nbsp;below.</p>
<p>The Graphcore <span class="caps">IPU</span> architecture is highly parallel with a large collection of
simple processors with small memories, connected by a high-bandwidth all-to-all
&#8216;exchange&#8217; interconnect. The architecture operates under a bulk-synchronous
parallel (<span class="caps">BSP</span>) model, whereby execution of a program proceeds as a sequence of
compute and exchange phases. Synchronisation is used to ensure all processes
are ready to start exchange. The <span class="caps">BSP</span> model is a powerful programming
abstraction because it precludes concurrency hazards, and <span class="caps">BSP</span> execution allows
the compute and exchange phases to make full use of the chip&#8217;s power resources.
Larger systems of <span class="caps">IPU</span> chips can be built by connecting the 10 inter-<span class="caps">IPU</span>&nbsp;links.</p>
<p>General&nbsp;details:</p>
<ul>
<li>16 nm, 23.6 bn transistors, ~800mm<sup>2</sup> die&nbsp;size.</li>
<li>1216 processor&nbsp;tiles.</li>
<li>125 TFLOPs peak <span class="caps">FP16</span> arithmetic with <span class="caps">FP32</span>&nbsp;accumulation.</li>
<li>300 <span class="caps">MB</span> total on-chip memory, distributed among processor cores, providing an
  aggregate access bandwidth of 45&nbsp;TBps.</li>
<li>All model state held on chip, there is no directly-attached <span class="caps">DRAM</span>.</li>
<li>150 W <span class="caps">TDP</span> (300 W PCIe&nbsp;card).</li>
</ul>
<p><span class="caps">IO</span>:</p>
<ul>
<li>2x PCIe-4 host <span class="caps">IO</span>&nbsp;links.</li>
<li>10x inter-card &#8216;<span class="caps">IPU</span>&nbsp;links&#8217;.</li>
<li>Total of 384 GBps <span class="caps">IO</span>&nbsp;bandwidth.</li>
</ul>
<p>Each&nbsp;core:</p>
<ul>
<li>Mixed-precision floating-point stochastic&nbsp;arithmetic.</li>
<li>Runs up to six program&nbsp;threads.</li>
</ul>
<p>References:</p>
<ul>
<li><a href="https://www.appg-ai.org/library/raais-2017-simon-knowles-co-founder-cto-graphcore/"><span class="caps">RAAIS</span> presentation, July&nbsp;2017</a></li>
<li><a href="https://cdn2.hubspot.net/hubfs/729091/NIPS2017/NIPS%2017%20-%20IPU.pdf?t=1526305355186"><span class="caps">NIPS</span> presentation, November&nbsp;2017</a></li>
<li><a href="https://cdn2.hubspot.net/hubfs/729091/assets/ScaledML%20Stanford%2024mar18%20SK.pdf">ScaledML presentation, March&nbsp;2018</a></li>
<li><a href="https://www.eetimes.com/document.asp?doc_id=1334578#">EETimes: Graphcore <span class="caps">CEO</span> Touts &#8216;Most Complex Processor&#8217; Ever, April&nbsp;2019</a></li>
<li><a href="https://www.servethehome.com/hands-on-with-a-graphcore-c2-ipu-pcie-card-at-dell-tech-world/">Serve the Home overview of the C2 card, June&nbsp;2019</a></li>
<li><a href="https://en.wikichip.org/wiki/graphcore/microarchitectures/colossus">WikiChip: Colossus&nbsp;microarchitecture</a></li>
</ul>
<p><a name="habana-gaudi" class="anchor"></a></p>
<h2>Habana Labs&nbsp;Gaudi</h2>
<p><img class="float-right img-fluid" src="thumbs/habana-gaudi_220x220.png" alt="Habana Gaudi board"></p>
<p>Habana&#8217;s Gaudi <span class="caps">AI</span> training processor shares similarities with contemporary
GPUs, particularly wide <span class="caps">SIMD</span> parallelism and <span class="caps">HBM2</span> memory. The chip integrates
ten 100G Ethernet links which support remote direct memory access (<span class="caps">RDMA</span>). This
<span class="caps">IO</span> capability allows large systems to be built with commodity networking
equipment, compared with Nvidia&#8217;s NVLink or&nbsp;OpenCAPI.</p>
<p>General&nbsp;details:</p>
<ul>
<li>Announced June&nbsp;2019.</li>
<li><span class="caps">TSMC</span> 16 nm with CoWoS, assumed die size ~500mm<sup>2</sup>.</li>
<li>Heterogeneous architecture with:<ul>
<li>a <span class="caps">GEMM</span> operations&nbsp;engine;</li>
<li>8 Tensor Processing Cores&nbsp;(TPCs);</li>
<li>a shared <span class="caps">SRAM</span> memory (software managed and accessible via <span class="caps">RDMA</span>).</li>
</ul>
</li>
<li>200W <span class="caps">TDP</span> for PCIe card and 300W <span class="caps">TDP</span> for the mezzanine&nbsp;card.</li>
<li>Unknown total on-chip&nbsp;memory.</li>
<li>Explicit memory management between chips (no&nbsp;coherency).</li>
</ul>
<p><span class="caps">TPC</span>&nbsp;core:</p>
<ul>
<li><span class="caps">VLIW</span> <span class="caps">SIMD</span> parallelism and a local <span class="caps">SRAM</span>&nbsp;memory.</li>
<li>Mixed precision: <span class="caps">FP32</span>, <span class="caps">BF16</span>, plus integer formats (<span class="caps">INT32</span>, <span class="caps">INT16</span>, <span class="caps">INT8</span>, <span class="caps">UINT32</span>, <span class="caps">UINT8</span>).</li>
<li>Random number&nbsp;generation.</li>
<li>Transcendental functions: Sigmoid, Tanh, Gaussian error linear unit&nbsp;(GeLU).</li>
<li>Tensor addressing and strided&nbsp;access.</li>
<li>Unknown local memory per <span class="caps">TPC</span>.</li>
</ul>
<p><span class="caps">IO</span>:</p>
<ul>
<li>4x <span class="caps">HBM2</span>-2000 <span class="caps">DRAM</span> stacks providing 32 <span class="caps">GB</span> at 1&nbsp;TBps.</li>
<li>10x 100GbE interfaces are integrated on-chip, supporting <span class="caps">RDMA</span> over Converged Ethernet (RoCE&nbsp;v2).</li>
<li>IOs are implemented with 20x 56 Gbps <span class="caps">PAM4</span> Tx/Rx SerDes and can also be configured as 20x 50 GbE. This allows up to 64 chips to be connected with non-blocking&nbsp;throughput.</li>
<li>PCIe-4 x16 host&nbsp;interface.</li>
</ul>
<p>References:</p>
<ul>
<li><a href="https://www.nextplatform.com/2019/06/17/ai-chip-startup-releases-training-accelerator-to-challenge-gpus">The Next Platform: <span class="caps">AI</span> Chip Startup Releases Training Accelerator to Challenge GPUs, June&nbsp;2019</a></li>
<li><a href="https://www.anandtech.com/show/14760/hot-chips-31-live-blogs-habanas-approach-to-ai-scaling">AnandTech report from <span class="caps">HC31</span>, August&nbsp;2019</a></li>
<li><a href="https://habana.ai/wp-content/uploads/2019/06/Habana-Gaudi-Training-Platform-whitepaper.pdf">Habana Labs Gaudi whitepaper, August&nbsp;2019</a></li>
</ul>
<p><a name="huawei-ascend" class="anchor"></a></p>
<h2>Huawei Ascend&nbsp;910</h2>
<p><img class="float-right img-fluid" src="thumbs/huawei-ascend_220x220.png" alt="Huawei Ascend floorplan"></p>
<p>Huawei&#8217;s Ascend also bears similarities to the latest GPUs with wide <span class="caps">SIMD</span>
arithmetic and a 3D matrix unit, comparable to Nvidia&#8217;s Tensor Cores, a
(assumed) coherent 32 <span class="caps">MB</span> shared L2 on-chip cache. The chip includes
additional logic for 128 channel video decoding engines for H.264/265. In their
Hot Chips presentation, Huawei described overlapping the cube and vector
operations to obtain high efficiency and the challenge of the memory hierarchy
with ratio of bandwidth to throughput dropping by 10x for L1 cache (in the
core), 100x for L2 cache (shared between cores), and 2000x for external <span class="caps">DRAM</span>.</p>
<p>General&nbsp;details:</p>
<ul>
<li>Announced August&nbsp;2019.</li>
<li>456 mm<sup>2</sup> logic die on a 7+ nm <span class="caps">EUV</span>&nbsp;process.</li>
<li>Copackaged with four 96 mm<sup>2</sup> <span class="caps">HBM2</span> stacks and &#8216;Nimbus&#8217; <span class="caps">IO</span> processor&nbsp;chip.</li>
<li>32 DaVinci&nbsp;cores.</li>
<li>Peak 256 TFLOPs (32 x 4096 x 2) <span class="caps">FP16</span> performance, double that for <span class="caps">INT8</span>.</li>
<li>32 <span class="caps">MB</span> shared on-chip <span class="caps">SRAM</span> (L2&nbsp;cache).</li>
<li>350W <span class="caps">TDP</span>.</li>
</ul>
<p>Interconnect and <span class="caps">IO</span>:</p>
<ul>
<li>Cores interconnected in a 6 x 4 2D mesh packet-switched network, providing
  128 GBps bidirectional bandwidth per&nbsp;core.</li>
<li>4 TBps access to L2&nbsp;cache.</li>
<li>1.2 TBps <span class="caps">HBM2</span> access&nbsp;bandwidth.</li>
<li>3x 30 GBps inter-chip&nbsp;IOs.</li>
<li>2x 25 GBps RoCE networking&nbsp;interfaces.</li>
</ul>
<p>Each DaVinci&nbsp;core:</p>
<ul>
<li>3D 16x16x16 matrix multiply unit providing 4,096 <span class="caps">FP16</span> MACs and 8,192 <span class="caps">INT8</span>&nbsp;MACs.</li>
<li>2,048 bit <span class="caps">SIMD</span> vector operations for <span class="caps">FP32</span> (x64), <span class="caps">FP16</span> (x128) and <span class="caps">INT8</span>&nbsp;(x256).</li>
<li>Support for scalar&nbsp;operations.</li>
</ul>
<p>References:</p>
<ul>
<li><a href="https://www.servethehome.com/huawei-ascend-910-provides-a-nvidia-ai-training-alternative/">Serve the Home report from <span class="caps">HC31</span>, August&nbsp;2019</a></li>
<li><a href="https://www.anandtech.com/show/14756/hot-chips-live-blogs-huawei-da-vinci-architecture">AnandTech report from <span class="caps">HC31</span>, August&nbsp;2019</a></li>
</ul>
<p><a name="intel-nnp-t" class="anchor"></a></p>
<h2>Intel <span class="caps">NNP</span>-T</h2>
<p><img class="float-right img-fluid" src="thumbs/intel-nnpt_220x220.png" alt="Intel NNP-T floorplan"></p>
<p>This chip is Intel&#8217;s second attempt at an accelerator for machine learning,
following the Xeon Phi. Like the <a href="#habana-gaudi">Habana Gaudi chip</a>, it
integrates a small number of wide vector cores, with <span class="caps">HBM2</span> integrated memory and
similar 100 Gbit <span class="caps">IO</span>&nbsp;links.</p>
<p>General&nbsp;details:</p>
<ul>
<li>27 bn&nbsp;transistors.</li>
<li>688 mm<sup>2</sup> die on <span class="caps">TSMC</span> <span class="caps">16FF</span>+ <span class="caps">TSMC</span> with&nbsp;CoWoS.</li>
<li>32 <span class="caps">GB</span> <span class="caps">HBM2</span>-2400 in four 8 <span class="caps">GB</span> stacks integrated on a 1200 mm<sup>2</sup> passive silicon&nbsp;interposer.</li>
<li>60 <span class="caps">MB</span> on-chip <span class="caps">SRAM</span> memory distributed among cores and <span class="caps">ECC</span>&nbsp;protected.</li>
<li>Up to 1.1 GHz core&nbsp;clock.</li>
<li>150-250W <span class="caps">TDP</span>.</li>
<li>24 Tensor Processing Cluster (<span class="caps">TCP</span>)&nbsp;cores.</li>
<li>TPCs connected in a 2D mesh network topology.<ul>
<li>Separate networks for different types of data: control, memory and inter-chip&nbsp;communication.</li>
<li>Support for&nbsp;multicast.</li>
</ul>
</li>
<li>119 TOPs peak&nbsp;performance.</li>
</ul>
<p><span class="caps">IO</span>:</p>
<ul>
<li>1.22 TBps <span class="caps">HBM2</span>&nbsp;bandwidth.</li>
<li>64 lanes of SerDes with peak 3.58 Tbps aggregate bandwidth (28 Gbps each direction in each lane) for inter-chip&nbsp;IOs.</li>
<li>x16 PCIe-4 host interface (also supports <span class="caps">OAM</span>, Open&nbsp;Compute).</li>
</ul>
<p><span class="caps">TPC</span>&nbsp;core:</p>
<ul>
<li>2x 32x32 BFloat16 multiplier arrays supporting <span class="caps">FMAC</span> operation with <span class="caps">FP32</span>&nbsp;accumulation.</li>
<li>Vector <span class="caps">FP32</span> and BFloat16 operations.<ul>
<li>Support for transcendental functions, random number generation, reductions and&nbsp;accumulations.</li>
<li>Programmable <span class="caps">FP32</span> lookup&nbsp;tables.</li>
</ul>
</li>
<li>A separate convolution engine for non-<span class="caps">MAC</span>&nbsp;compute.</li>
<li>2.5 <span class="caps">MB</span> two-port private memory with 1.4 TBps read/write&nbsp;bandwidth.</li>
<li>Memory supports tensor transpose&nbsp;operation.</li>
<li>Communication interface supporting dynamic packet routing on the mesh
  (virtual channel, reliable&nbsp;transmission).</li>
</ul>
<p>Scaling:</p>
<ul>
<li>Up to 1024 chips with direct interconnections, providing the same
  distributed-memory programming model (explicit memory management,
  synchronisation primitives, message&nbsp;passing).</li>
<li>Scaling demonstrated up to 32 chips connected in a ring&nbsp;topology.</li>
</ul>
<p>References:</p>
<ul>
<li><a href="https://fuse.wikichip.org/news/2219/intels-spring-crest-nnp-l-initial-details/">WikiChip: Intel’s Spring Crest <span class="caps">NNP</span>-L Initial Details, April&nbsp;2019</a></li>
<li><a href="https://www.tomshardware.com/news/intel-nervana-nueral-net-processor-nnt-p,40185.html">Tom&#8217;s Hardware, August&nbsp;2019</a></li>
<li><a href="https://newsroom.intel.com/wp-content/uploads/sites/11/2019/08/Intel-Nervana-NNP-T-HotChips-presentation.pdf">Intel&#8217;s <span class="caps">HC31</span> presentation (<span class="caps">PDF</span>), August&nbsp;2019</a></li>
<li><a href="https://www.anandtech.com/show/14757/hot-chips-live-blogs-intel-spring-crest-nnpt-on-16nm-tsmc">AnandTech report from <span class="caps">HC31</span>, August&nbsp;2019</a></li>
</ul>
<p><a name="nvidia-volta" class="anchor"></a></p>
<h2>Nvidia&nbsp;Volta</h2>
<p><img class="float-right img-fluid" src="thumbs/nvidia-volta_220x220.png" alt="Nvidia Volta board"></p>
<p>Volta introduces Tensor Cores, <span class="caps">HBM2</span> and NVLink 2.0, from the <a href="https://en.wikipedia.org/wiki/Pascal_(microarchitecture)">Pascal
architecture</a>.</p>
<p>General&nbsp;details:</p>
<ul>
<li>Announced May&nbsp;2017.</li>
<li>815 mm<sup>2</sup> on <span class="caps">TSMC</span> 12nm <span class="caps">FFN</span>, 21.1 bn&nbsp;transistors.</li>
<li>300 W <span class="caps">TDP</span> (<span class="caps">SXM2</span> form&nbsp;factor).</li>
<li>6 <span class="caps">MB</span> L2&nbsp;cache.</li>
<li>84 SMs, each containing: 64 <span class="caps">FP32</span> <span class="caps">CUDA</span> cores, 32 <span class="caps">FP64</span> <span class="caps">CUDA</span> cores and 8 Tensor
  Cores (5376 <span class="caps">FP32</span> cores, 2688 <span class="caps">FP64</span> cores, 672&nbsp;TCs).</li>
<li>Tensor Cores perform 4x4 <span class="caps">FMA</span>, achieving 64 <span class="caps">FMA</span> ops/cycle, and 128&nbsp;FLOPs.</li>
<li>128 <span class="caps">KB</span> L1 data cache/shared memory and four 16K 32-bit registers per <span class="caps">SM</span>.</li>
</ul>
<p><span class="caps">IO</span>:</p>
<ul>
<li>32 <span class="caps">GB</span> <span class="caps">HBM2</span> <span class="caps">DRAM</span>, at 900 GBps&nbsp;bandwidth.</li>
<li>NVLink 2.0 at 300&nbsp;GBps.</li>
</ul>
<p>References:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Volta_(microarchitecture)">Wikipedia: Volta&nbsp;(microarchitecture)</a></li>
<li><a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf">Volta architecture&nbsp;whitepaper</a></li>
<li><a href="https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-datasheet-letter-fnl-web.pdf">Nvidia Tesla V100&nbsp;datasheet</a></li>
<li><a href="https://www.anandtech.com/show/11367/nvidia-volta-unveiled-gv100-gpu-and-tesla-v100-accelerator-announced">AnandTech - <span class="caps">NVIDIA</span> Volta Unveiled: <span class="caps">GV100</span> <span class="caps">GPU</span> and Tesla V100 Accelerator Announced, May&nbsp;2017</a></li>
</ul>
<p><a name="nvidia-turing" class="anchor"></a></p>
<h2>Nvidia&nbsp;Turing</h2>
<p><img class="float-right img-fluid" src="thumbs/nvidia-turing_220x220.png" alt="Nvidia Turing die shot"></p>
<p>Turing is an architectural revision of Volta, manufactured on the same 16 nm
process, but with fewer <span class="caps">CUDA</span> and Tensor cores. It consequently has a smaller
die size and lower power envelope. Apart from <span class="caps">ML</span> tasks, it is designed to
perform real-time ray tracing, for which it also used the Tensor&nbsp;Cores.</p>
<p>General&nbsp;details:</p>
<ul>
<li>Announced September&nbsp;2018.</li>
<li><span class="caps">TSMC</span> 12nm <span class="caps">FFN</span>, 754 mm<sup>2</sup> die, 18.6 bn&nbsp;transistors.</li>
<li>260 W <span class="caps">TDP</span>.</li>
<li>72 SMs, each containing: 64 <span class="caps">FP32</span> cores, and 64 <span class="caps">INT32</span> cores, 8 Tensor cores
  (4608 <span class="caps">FP32</span> cores, 4608 <span class="caps">INT32</span> cores and 576&nbsp;TCs).</li>
<li>Peak performance with boost clock: 16.3 TFLOPs <span class="caps">FP32</span>, 130.5 TFLOPs <span class="caps">FP16</span>, 261 TFLOPs <span class="caps">INT8</span>, 522 TFLOPs <span class="caps">INT4</span>.</li>
<li>24.5 <span class="caps">MB</span> on-chip memory between 6 <span class="caps">MB</span> L2 cache and 256 <span class="caps">KB</span> <span class="caps">SM</span> register&nbsp;files.</li>
<li>1455 MHz base&nbsp;clock.</li>
</ul>
<p><span class="caps">IO</span>:</p>
<ul>
<li>12x 32-bit <span class="caps">GDDR6</span> memory providing 672 GBps aggregate&nbsp;bandwidth.</li>
<li>2x NVLink x8 links, each providing up to 26 GBps&nbsp;bidirectional.</li>
</ul>
<p>References:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Turing_(microarchitecture)">Wikipedia: Turing&nbsp;(mircoarchitecture)</a></li>
<li><a href="https://www.anandtech.com/show/13214/nvidia-reveals-next-gen-turing-gpu-architecture">AnandTech: <span class="caps">NVIDIA</span> Reveals Next-Gen Turing <span class="caps">GPU</span> Architecture, August&nbsp;2018</a></li>
<li><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf">Nvidia Turing <span class="caps">GPU</span> architecture whitepaper (<span class="caps">PDF</span>)</a></li>
</ul>
<h2>Further&nbsp;reading</h2>
<ul>
<li><a href="https://github.com/basicmi/AI-Chip"><span class="caps">AI</span> Chip - A list of ICs and IPs for <span class="caps">AI</span>, Machine Learning and Deep Learning&nbsp;(GitHub)</a></li>
<li><a href="https://mlperf.org/training-results-0-6/">MLPerf Training v0.6 results (<span class="caps">TPU</span> v3 and Volta&nbsp;only)</a></li>
</ul>
<p>See
<a href="https://news.ycombinator.com/item?id=21178609">this thread on Hacker News</a>
for discussion of this&nbsp;note.</p>
<!--
More:
- SambaNova (no sign of silicon)
- Wave Computing
- Cambricon (Huwawei NPU, Alibaba)
- Baidu XPU
- Cornami
- Fijitsu DPU
- IBM TrueNorth
- ThinCI
- Vathys
- Groq (no details yet)
Inference only:
- Qualcomm Cloud AI 100 chip
- Tesla Neural Processor
- FlexLogic
- Alibaba Hanguang 800 (12nm 17bn transistors)
- T-Head XuanTie 910
Brain inspired:
- Mythic (analoge computing in NAND memory, works on 8 bit values only)
- Applied Brain Research (ABR) - Nengo "brain" chip
- Rain Neuromophic
- GrAI (pronounced "gray") Matter Labs has a fully digital neuromorphic processor
-->
  </div>
  <div class="article-footer">
    <p>Please get in touch (mail @ this domain) with any
    comments, corrections or suggestions.</p>
  </div>
  </div>
  </main>

  <hr>
  <footer class="text-muted">
    <div class="container">
      <!--<p class="float-right" style="padding-left:1em">
        <a href="https://github.com/jameshanlon">
        <span>
          <svg viewBox="0 0 16 16" width="16px" height="16px">
            <path fill="#828282"
               d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
               c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
               c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
               c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
               C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
               c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
               c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
               c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
               c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z">
            </path>
          </svg>
        </span>
        <span>jameshanlon</span><br>
        </a>
        <a href="https://twitter.com/jameswhanlon">
          <span>
            <svg viewBox="0 0 16 16" width="16px" height="16px">
              <path fill="#828282"
                d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z">
              </path>
            </svg>
          </span>
          <span class="username">@jameswhanlon</span>
        </a>
      </p>
      <p>-->
      <div class="small">
        <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
          <img alt="Creative Commons Licence" style="border-width:0"
               src="https://i.creativecommons.org/l/by/4.0/80x15.png" />
        </a><br>
        Unless otherwise noted, all content is freely available under a
        <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
          Creative Commons Attribution 4.0 International License</a>.<br>
        Subscribe: <a href="http://jameswhanlon.com/reeds/atom.xml">Atom</a> /
        <a href="http://jameswhanlon.com/reeds/rss.xml">RSS</a>
      </div>
    </div>
  </footer>
  <script src="./theme/js/jquery-3.6.0.min.js"></script>
  <script src="./theme/js/popper.min.js"></script>
  <script src="./theme/js/bootstrap.min.js"></script>
  <script src="./theme/js/lightbox.min.js"></script>
  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <script src="./theme/js/ie10-viewport-bug-workaround.js"></script>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-19458374-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>